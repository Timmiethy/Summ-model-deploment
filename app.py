import streamlit as st
import os
import re
import time
import torch
import docx
from transformers import T5ForConditionalGeneration, T5Tokenizer

# CÃ¡c thÆ° viá»‡n cho chá»©c nÄƒng Há»i-ÄÃ¡p (RAG)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ==============================================================================
# PHáº¦N Cáº¤U HÃŒNH VÃ€ Táº¢I MODEL
# ==============================================================================

# THAY Äá»”I: HÃ£y Ä‘iá»n Ä‘Ãºng ID model cá»§a báº¡n trÃªn Hugging Face Hub
SUMMARIZER_MODEL_PATH = 'Timmiethy/t5-legal-summarizer-final' 
EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"
LLM_MODEL_NAME = "gemini-1.5-flash-latest"

# Láº¥y Google API Key tá»« Streamlit Secrets
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY
except (KeyError, FileNotFoundError):
    st.error("Lá»–I: KhÃ´ng tÃ¬m tháº¥y GOOGLE_API_KEY. Vui lÃ²ng thiáº¿t láº­p trong Streamlit Secrets.")
    GOOGLE_API_KEY = None

@st.cache_resource
def load_summarizer_model():
    """Táº£i tokenizer vÃ  model T5 trá»±c tiáº¿p tá»« Hugging Face."""
    st.info("Äang táº£i mÃ´ hÃ¬nh TÃ³m táº¯t vÄƒn báº£n... Vui lÃ²ng chá».")
    try:
        tokenizer = T5Tokenizer.from_pretrained(SUMMARIZER_MODEL_PATH)
        model = T5ForConditionalGeneration.from_pretrained(SUMMARIZER_MODEL_PATH)
        st.success("Táº£i mÃ´ hÃ¬nh TÃ³m táº¯t thÃ nh cÃ´ng!")
        return tokenizer, model # Tráº£ vá» cáº£ tokenizer vÃ  model
    except Exception as e:
        st.error(f"Lá»—i khi táº£i mÃ´ hÃ¬nh tÃ³m táº¯t: {e}")
        return None, None

@st.cache_resource
def load_embedding_model():
    """Táº£i mÃ´ hÃ¬nh embedding cho RAG."""
    st.info("Äang táº£i mÃ´ hÃ¬nh Embedding cho Há»i-ÄÃ¡p... Vui lÃ²ng chá».")
    try:
        embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)
        st.success("Táº£i mÃ´ hÃ¬nh Embedding thÃ nh cÃ´ng!")
        return embeddings
    except Exception as e:
        st.error(f"Lá»—i khi táº£i mÃ´ hÃ¬nh embedding: {e}")
        return None

# ==============================================================================
# CÃC HÃ€M LOGIC
# ==============================================================================

def extract_key_sections(full_text: str) -> str:
    """HÃ m trÃ­ch lá»c ná»™i dung chÃ­nh cá»§a báº£n Ã¡n."""
    start_patterns = [
        r"Ná»˜I DUNG Vá»¤ ÃN\s*:?", r"NHáº¬N THáº¤Y\s*:?", r"XÃ‰T THáº¤Y\s*:?",
        r"NHáº¬N Äá»ŠNH Cá»¦A TÃ’A ÃN\s*:?", r"NHáº¬N Äá»ŠNH Cá»¦A Há»˜I Äá»’NG XÃ‰T Xá»¬\s*:?",
    ]
    start_index = -1
    for pattern in start_patterns:
        match = re.search(pattern, full_text, re.IGNORECASE)
        if match:
            index = match.start()
            if start_index == -1 or index < start_index:
                start_index = index
    if start_index != -1:
        return full_text[start_index:]
    return full_text

def summarize_text(tokenizer, model, clean_text):
    """HÃ m gá»i model Ä‘á»ƒ tÃ³m táº¯t, sá»­ dá»¥ng thÆ° viá»‡n transformers."""
    if not clean_text.strip():
        return "VÄƒn báº£n trá»‘ng, khÃ´ng cÃ³ gÃ¬ Ä‘á»ƒ tÃ³m táº¯t."
    
    final_text = re.sub(r"\s+", " ", clean_text).strip()
    prefixed_text = "summarize: " + final_text
    
    with st.spinner('MÃ´ hÃ¬nh Ä‘ang tÃ³m táº¯t...'):
        input_ids = tokenizer.encode(prefixed_text, return_tensors="pt", max_length=1024, truncation=True)
        summary_ids = model.generate(input_ids, max_length=256, num_beams=5, early_stopping=True)
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
    return summary

def setup_qa_chain_from_text(documents_text, embeddings_model):
    """HÃ m xÃ¢y dá»±ng há»‡ thá»‘ng QA tá»« danh sÃ¡ch cÃ¡c Ä‘oáº¡n vÄƒn báº£n."""
    if not GOOGLE_API_KEY:
        st.error("KhÃ´ng thá»ƒ thiáº¿t láº­p chuá»—i QA vÃ¬ thiáº¿u Google API Key.")
        return None
        
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    texts = text_splitter.create_documents(documents_text)
    db = FAISS.from_documents(texts, embeddings_model)
    prompt_template = """
    Báº¡n lÃ  má»™t trá»£ lÃ½ phÃ¡p lÃ½ AI chÃ­nh xÃ¡c. Chá»‰ sá»­ dá»¥ng thÃ´ng tin trong 'Ngá»¯ cáº£nh' dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    Tuyá»‡t Ä‘á»‘i khÃ´ng bá»‹a Ä‘áº·t hoáº·c dÃ¹ng kiáº¿n thá»©c ngoÃ i.
    Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y tráº£ lá»i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin vá» váº¥n Ä‘á» nÃ y trong cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p."

    Ngá»¯ cáº£nh: {context}
    CÃ¢u há»i: {question}
    Tráº£ lá»i (báº±ng tiáº¿ng Viá»‡t):
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME, temperature=0.2)
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff", retriever=db.as_ retriever(search_kwargs={"k": 4}),
        chain_type_kwargs={"prompt": PROMPT}, return_source_documents=True
    )
    return qa_chain

# ==============================================================================
# GIAO DIá»†N WEB STREAMLIT
# ==============================================================================

st.set_page_config(page_title="Trá»£ LÃ½ PhÃ¡p LÃ½ AI", layout="wide")
st.title("âš–ï¸ Trá»£ LÃ½ PhÃ¡p LÃ½ AI")
st.write("Cung cáº¥p bá»Ÿi **Timmiethy** - á»¨ng dá»¥ng AI há»— trá»£ tÃ³m táº¯t vÃ  há»i Ä‘Ã¡p vÄƒn báº£n phÃ¡p lÃ½.")

# Táº£i cÃ¡c model
tokenizer, summarizer_model = load_summarizer_model()
embeddings = load_embedding_model()

tab1, tab2 = st.tabs(["ğŸ“ TÃ³m Táº¯t VÄƒn Báº£n", "ğŸ’¬ Há»i-ÄÃ¡p PhÃ¡p LÃ½ (RAG)"])

# --- Tab 1: TÃ³m táº¯t vÄƒn báº£n ---
with tab1:
    st.header("TÃ³m Táº¯t Báº£n Ãn Tá»± Äá»™ng")
    st.write("Táº£i lÃªn má»™t file báº£n Ã¡n (.docx) Ä‘á»ƒ Ä‘Æ°á»£c tiá»n xá»­ lÃ½ vÃ  tÃ³m táº¯t tá»± Ä‘á»™ng.")

    uploaded_file_summary = st.file_uploader("Táº£i lÃªn file .docx cá»§a báº¡n", type=['docx'])
    
    if uploaded_file_summary is not None:
        try:
            st.info("Äang Ä‘á»c file...")
            doc = docx.Document(uploaded_file_summary)
            raw_text = "\n".join([para.text for para in doc.paragraphs])
            st.success("Äá»c file thÃ nh cÃ´ng!")

            st.info("Äang tiá»n xá»­ lÃ½ vÄƒn báº£n...")
            processed_text = extract_key_sections(raw_text)
            st.success("Tiá»n xá»­ lÃ½ hoÃ n táº¥t!")

            st.text_area("Ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch lá»c:", value=processed_text, height=250)
            
            if st.button("Táº¡o TÃ³m Táº¯t"):
                if summarizer_model and tokenizer:
                    summary_result = summarize_text(tokenizer, summarizer_model, processed_text)
                    st.subheader("Báº£n tÃ³m táº¯t:")
                    st.write(summary_result)
                else:
                    st.error("MÃ´ hÃ¬nh tÃ³m táº¯t chÆ°a Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng. Vui lÃ²ng kiá»ƒm tra láº¡i logs.")
        except Exception as e:
            st.error(f"ÄÃ£ cÃ³ lá»—i xáº£y ra: {e}")
    else:
        st.info("Vui lÃ²ng táº£i lÃªn má»™t file .docx Ä‘á»ƒ báº¯t Ä‘áº§u.")

# --- Tab 2: Há»i-Ä‘Ã¡p ---
with tab2:
    st.header("Há»i-ÄÃ¡p Dá»±a TrÃªn TÃ i Liá»‡u")
    st.write("Táº£i lÃªn má»™t hoáº·c nhiá»u file vÄƒn báº£n (.txt) Ä‘á»ƒ lÃ m cÆ¡ sá»Ÿ kiáº¿n thá»©c, sau Ä‘Ã³ Ä‘áº·t cÃ¢u há»i.")

    uploaded_files_qa = st.file_uploader("Táº£i lÃªn cÃ¡c file .txt cá»§a báº¡n", type="txt", accept_multiple_files=True)

    if uploaded_files_qa:
        documents_content = [file.read().decode("utf-8") for file in uploaded_files_qa]
        st.success(f"ÄÃ£ táº£i lÃªn vÃ  xá»­ lÃ½ {len(uploaded_files_qa)} file.")
        
        # Sá»­ dá»¥ng session_state Ä‘á»ƒ lÆ°u trá»¯ chuá»—i QA, trÃ¡nh build láº¡i má»—i láº§n tÆ°Æ¡ng tÃ¡c
        if 'qa_chain' not in st.session_state or st.session_state.get('last_uploaded_files') != [f.name for f in uploaded_files_qa]:
            with st.spinner("Äang xÃ¢y dá»±ng cÆ¡ sá»Ÿ tri thá»©c tá»« tÃ i liá»‡u..."):
                st.session_state.qa_chain = setup_qa_chain_from_text(documents_content, embeddings)
                st.session_state.last_uploaded_files = [f.name for f in uploaded_files_qa]

        if 'qa_chain' in st.session_state and st.session_state.qa_chain:
            st.info("Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng. HÃ£y Ä‘áº·t cÃ¢u há»i cá»§a báº¡n.")
            question = st.text_input("CÃ¢u há»i cá»§a báº¡n:")

            if question:
                with st.spinner("Äang tÃ¬m kiáº¿m cÃ¢u tráº£ lá»i..."):
                    result = st.session_state.qa_chain.invoke({"query": question})
                    st.subheader("âœ… CÃ¢u tráº£ lá»i:")
                    st.write(result["result"])
                    with st.expander("ğŸ” Xem cÃ¡c nguá»“n trÃ­ch dáº«n"):
                        for doc in result["source_documents"]:
                            st.write(f"**TrÃ­ch tá»«:** ...{doc.page_content[100:400].strip()}...")
