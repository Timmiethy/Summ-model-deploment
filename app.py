# app.py (tiáº¿p theo vÃ  hoÃ n chá»‰nh)

def summarize_text(tokenizer, model, clean_text):
    """HÃ m gá»i model Ä‘á»ƒ tÃ³m táº¯t, sá»­ dá»¥ng thÆ° viá»‡n transformers."""
    if not clean_text.strip():
        return "Vui lÃ²ng nháº­p vÄƒn báº£n cáº§n tÃ³m táº¯t."
    
    # Chuáº©n bá»‹ vÄƒn báº£n Ä‘áº§u vÃ o cho model T5
    final_text = re.sub(r"\s+", " ", clean_text).strip()
    prefixed_text = "summarize: " + final_text
    
    with st.spinner('MÃ´ hÃ¬nh Ä‘ang tÃ³m táº¯t...'):
        # 1. MÃ£ hÃ³a vÄƒn báº£n thÃ nh cÃ¡c ID
        input_ids = tokenizer.encode(prefixed_text, return_tensors="pt", max_length=1024, truncation=True)
        
        # 2. Táº¡o báº£n tÃ³m táº¯t
        summary_ids = model.generate(input_ids, max_length=256, num_beams=4, early_stopping=True)
        
        # 3. Giáº£i mÃ£ cÃ¡c ID thÃ nh vÄƒn báº£n
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
    return summary

def setup_qa_chain_from_text(documents_text, embeddings_model):
    """HÃ m xÃ¢y dá»±ng há»‡ thá»‘ng QA tá»« danh sÃ¡ch cÃ¡c Ä‘oáº¡n vÄƒn báº£n."""
    # (HÃ m nÃ y giá»¯ nguyÃªn, khÃ´ng cáº§n thay Ä‘á»•i)
    if not GOOGLE_API_KEY:
        st.error("KhÃ´ng thá»ƒ thiáº¿t láº­p chuá»—i QA vÃ¬ thiáº¿u Google API Key.")
        return None
        
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    texts = text_splitter.create_documents(documents_text)
    
    db = FAISS.from_documents(texts, embeddings_model)

    prompt_template = """
    Báº¡n lÃ  má»™t trá»£ lÃ½ phÃ¡p lÃ½ AI chÃ­nh xÃ¡c. Chá»‰ sá»­ dá»¥ng thÃ´ng tin trong 'Ngá»¯ cáº£nh' dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    Tuyá»‡t Ä‘á»‘i khÃ´ng bá»‹a Ä‘áº·t hoáº·c dÃ¹ng kiáº¿n thá»©c ngoÃ i.
    Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y tráº£ lá»i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin vá» váº¥n Ä‘á» nÃ y trong cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p."

    Ngá»¯ cáº£nh:
    {context}

    CÃ¢u há»i: {question}

    Tráº£ lá»i (báº±ng tiáº¿ng Viá»‡t):
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

    llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME, temperature=0.2)
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 4}),
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True
    )
    return qa_chain

# ==============================================================================
# GIAO DIá»†N WEB STREAMLIT
# ==============================================================================

st.set_page_config(page_title="Trá»£ LÃ½ PhÃ¡p LÃ½ AI", layout="wide")
st.title("âš–ï¸ Trá»£ LÃ½ PhÃ¡p LÃ½ AI")

# Táº£i cÃ¡c model
# --- THAY Äá»”I 3: Nháº­n vá» cáº£ tokenizer vÃ  model ---
tokenizer, summarizer_model = load_summarizer_model()
embeddings = load_embedding_model()

tab1, tab2 = st.tabs(["ğŸ“ TÃ³m Táº¯t VÄƒn Báº£n", "ğŸ’¬ Há»i-ÄÃ¡p PhÃ¡p LÃ½ (RAG)"])

# --- Tab 1: TÃ³m táº¯t vÄƒn báº£n ---
with tab1:
    st.header("TÃ³m Táº¯t Báº£n Ãn Tá»± Äá»™ng")
    st.write("Táº£i lÃªn má»™t file báº£n Ã¡n (.docx) Ä‘á»ƒ Ä‘Æ°á»£c tiá»n xá»­ lÃ½ vÃ  tÃ³m táº¯t tá»± Ä‘á»™ng.")

    uploaded_file_summary = st.file_uploader(
        "Táº£i lÃªn file .docx cá»§a báº¡n",
        type=['docx']
    )
    
    if uploaded_file_summary is not None:
        try:
            st.info("Äang Ä‘á»c file...")
            doc = docx.Document(uploaded_file_summary)
            raw_text = "\n".join([para.text for para in doc.paragraphs])
            st.success("Äá»c file thÃ nh cÃ´ng!")

            st.info("Äang tiá»n xá»­ lÃ½ vÄƒn báº£n...")
            processed_text = extract_key_sections(raw_text)
            st.success("Tiá»n xá»­ lÃ½ hoÃ n táº¥t! VÄƒn báº£n Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ tÃ³m táº¯t.")

            st.text_area("Ná»™i dung Ä‘Ã£ Ä‘Æ°á»£c tiá»n xá»­ lÃ½:", value=processed_text, height=250)
            
            if st.button("Táº¡o TÃ³m Táº¯t"):
                # Kiá»ƒm tra xem model vÃ  tokenizer Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng chÆ°a
                if summarizer_model and tokenizer:
                    # --- THAY Äá»”I 4: Truyá»n cáº£ tokenizer vÃ  model vÃ o hÃ m ---
                    summary_result = summarize_text(tokenizer, summarizer_model, processed_text)
                    st.subheader("Báº£n tÃ³m táº¯t:")
                    st.write(summary_result)
                else:
                    st.error("MÃ´ hÃ¬nh tÃ³m táº¯t chÆ°a Ä‘Æ°á»£c táº£i.")

        except Exception as e:
            st.error(f"ÄÃ£ cÃ³ lá»—i xáº£y ra: {e}")
    else:
        st.info("Vui lÃ²ng táº£i lÃªn má»™t file .docx Ä‘á»ƒ báº¯t Ä‘áº§u.")

# --- Tab 2: Há»i-Ä‘Ã¡p ---
with tab2:
    st.header("Há»i-ÄÃ¡p Dá»±a TrÃªn TÃ i Liá»‡u")
    st.write("Táº£i lÃªn má»™t hoáº·c nhiá»u file vÄƒn báº£n (.txt) Ä‘á»ƒ lÃ m cÆ¡ sá»Ÿ kiáº¿n thá»©c, sau Ä‘Ã³ Ä‘áº·t cÃ¢u há»i vá» ná»™i dung cá»§a chÃºng.")

    uploaded_files = st.file_uploader(
        "Táº£i lÃªn cÃ¡c file .txt cá»§a báº¡n",
        type="txt",
        accept_multiple_files=True
    )

    if uploaded_files:
        documents_content = [file.read().decode("utf-8") for file in uploaded_files]
        st.success(f"ÄÃ£ táº£i lÃªn vÃ  xá»­ lÃ½ {len(uploaded_files)} file.")
        
        @st.cache_data
        def get_qa_chain(_docs_content):
            with st.spinner("Äang xÃ¢y dá»±ng cÆ¡ sá»Ÿ tri thá»©c tá»« tÃ i liá»‡u..."):
                return setup_qa_chain_from_text(_docs_content, embeddings)

        qa_chain = get_qa_chain(tuple(documents_content))

        if qa_chain:
            st.info("Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng. HÃ£y Ä‘áº·t cÃ¢u há»i cá»§a báº¡n vÃ o Ã´ bÃªn dÆ°á»›i.")
            question = st.text_input("CÃ¢u há»i cá»§a báº¡n:")

            if question:
                with st.spinner("Äang tÃ¬m kiáº¿m cÃ¢u tráº£ lá»i..."):
                    result = qa_chain.invoke({"query": question})
                    st.subheader("âœ… CÃ¢u tráº£ lá»i:")
                    st.write(result["result"])
