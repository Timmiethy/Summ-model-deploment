# app.py

import streamlit as st
import os
import re
import time
import torch

# --- CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t ---
# NgÆ°á»i dÃ¹ng sáº½ cáº§n cÃ i Ä‘áº·t chÃºng thÃ´ng qua requirements.txt
from simpletransformers.t5 import T5Model
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ==============================================================================
# PHáº¦N Cáº¤U HÃŒNH VÃ€ Táº¢I MODEL (QUAN TRá»ŒNG: CACHING)
# ==============================================================================

# --- ÄÆ°á»ng dáº«n tá»›i cÃ¡c model Ä‘Ã£ huáº¥n luyá»‡n ---
# LÆ¯U Ã: Khi deploy, báº¡n cáº§n Ä‘áº£m báº£o cÃ¡c file model nÃ y cÃ³ sáºµn trÃªn server.
# CÃ¡ch tá»‘t nháº¥t lÃ  táº£i chÃºng lÃªn Hugging Face Hub vÃ  táº£i vá» tá»« Ä‘Ã³.
# á» Ä‘Ã¢y, ta giáº£ Ä‘á»‹nh chÃºng náº±m trong thÆ° má»¥c 'models'.
SUMMARIZER_MODEL_PATH = "Timmiethy/t5-legal-summarizer-final"
EMBEDDING_MODEL_NAME = "bkai-foundation-models/vietnamese-bi-encoder"
LLM_MODEL_NAME = "gemini-1.5-flash-latest"

# Láº¥y Google API Key tá»« Streamlit Secrets
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
except KeyError:
    st.error(
        "Lá»–I: KhÃ´ng tÃ¬m tháº¥y GOOGLE_API_KEY. Vui lÃ²ng thiáº¿t láº­p trong Streamlit Secrets."
    )
    GOOGLE_API_KEY = None  # GÃ¡n lÃ  None Ä‘á»ƒ trÃ¡nh lá»—i khi cháº¡y


# --- Sá»­ dá»¥ng cache cá»§a Streamlit Ä‘á»ƒ khÃ´ng pháº£i táº£i láº¡i model má»—i khi ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tÃ¡c ---
@st.cache_resource
def load_summarizer_model():
    """Táº£i mÃ´ hÃ¬nh tÃ³m táº¯t T5."""
    st.info("Äang táº£i mÃ´ hÃ¬nh TÃ³m táº¯t vÄƒn báº£n... Vui lÃ²ng chá».")
    if not os.path.exists(SUMMARIZER_MODEL_PATH):
        return None
    try:
        model = T5Model("t5", SUMMARIZER_MODEL_PATH, use_cuda=torch.cuda.is_available())
        return model
    except Exception as e:
        st.error(f"Lá»—i khi táº£i mÃ´ hÃ¬nh tÃ³m táº¯t: {e}")
        return None


@st.cache_resource
def load_embedding_model():
    """Táº£i mÃ´ hÃ¬nh embedding cho RAG."""
    st.info("Äang táº£i mÃ´ hÃ¬nh Embedding cho Há»i-ÄÃ¡p... Vui lÃ²ng chá».")
    try:
        embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL_NAME)
        return embeddings
    except Exception as e:
        st.error(f"Lá»—i khi táº£i mÃ´ hÃ¬nh embedding: {e}")
        return None


# ==============================================================================
# CÃC HÃ€M LOGIC Tá»ª CODE Cá»¦A Báº N
# ==============================================================================


def extract_key_sections(full_text: str) -> str:
    """HÃ m trÃ­ch lá»c ná»™i dung chÃ­nh cá»§a báº£n Ã¡n."""
    start_patterns = [
        r"Ná»˜I DUNG Vá»¤ ÃN\s*:?",
        r"NHáº¬N THáº¤Y\s*:?",
        r"XÃ‰T THáº¤Y\s*:?",
        r"NHáº¬N Äá»ŠNH Cá»¦A TÃ’A ÃN\s*:?",
        r"NHáº¬N Äá»ŠNH Cá»¦A Há»˜I Äá»’NG XÃ‰T Xá»¬\s*:?",
    ]
    start_index = -1
    for pattern in start_patterns:
        match = re.search(pattern, full_text, re.IGNORECASE)
        if match:
            index = match.start()
            if start_index == -1 or index < start_index:
                start_index = index
    if start_index != -1:
        return full_text[start_index:]
    return full_text


def summarize_text(model, text_to_summarize):
    """HÃ m gá»i model Ä‘á»ƒ tÃ³m táº¯t."""
    if not text_to_summarize.strip():
        return "Vui lÃ²ng nháº­p vÄƒn báº£n cáº§n tÃ³m táº¯t."

    # Tiá»n xá»­ lÃ½ vÃ  lÃ m sáº¡ch vÄƒn báº£n Ä‘áº§u vÃ o
    processed_text = extract_key_sections(text_to_summarize)
    final_cleaned_text = re.sub(r"\s+", " ", processed_text).strip()

    # ThÃªm prefix theo yÃªu cáº§u cá»§a model T5
    prefixed_text = "summarize: " + final_cleaned_text

    with st.spinner("MÃ´ hÃ¬nh Ä‘ang tÃ³m táº¯t..."):
        summary = model.predict([prefixed_text])
    return summary[0]


def setup_qa_chain_from_text(documents_text, embeddings_model):
    """HÃ m xÃ¢y dá»±ng há»‡ thá»‘ng QA tá»« danh sÃ¡ch cÃ¡c Ä‘oáº¡n vÄƒn báº£n."""
    if not GOOGLE_API_KEY:
        st.error("KhÃ´ng thá»ƒ thiáº¿t láº­p chuá»—i QA vÃ¬ thiáº¿u Google API Key.")
        return None

    # 2. PhÃ¢n máº£nh vÄƒn báº£n
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    texts = text_splitter.create_documents(documents_text)

    # 3. XÃ¢y dá»±ng cÆ¡ sá»Ÿ dá»¯ liá»‡u vector
    db = FAISS.from_documents(texts, embeddings_model)

    # 4. Äá»‹nh nghÄ©a prompt
    prompt_template = """
    Báº¡n lÃ  má»™t trá»£ lÃ½ phÃ¡p lÃ½ AI chÃ­nh xÃ¡c. Chá»‰ sá»­ dá»¥ng thÃ´ng tin trong 'Ngá»¯ cáº£nh' dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    Tuyá»‡t Ä‘á»‘i khÃ´ng bá»‹a Ä‘áº·t hoáº·c dÃ¹ng kiáº¿n thá»©c ngoÃ i.
    Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, hÃ£y tráº£ lá»i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin vá» váº¥n Ä‘á» nÃ y trong cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p."

    Ngá»¯ cáº£nh:
    {context}

    CÃ¢u há»i: {question}

    Tráº£ lá»i (báº±ng tiáº¿ng Viá»‡t):
    """
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    # 5. Thiáº¿t láº­p LLM vÃ  chuá»—i QA
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME, temperature=0.2)

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={"k": 4}),
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True,
    )
    return qa_chain


# ==============================================================================
# GIAO DIá»†N WEB STREAMLIT
# ==============================================================================

st.set_page_config(page_title="Trá»£ LÃ½ PhÃ¡p LÃ½ AI", layout="wide")

st.title("âš–ï¸ Trá»£ LÃ½ PhÃ¡p LÃ½ AI")
st.write(
    "á»¨ng dá»¥ng nÃ y sá»­ dá»¥ng AI Ä‘á»ƒ tÃ³m táº¯t báº£n Ã¡n vÃ  tráº£ lá»i cÃ¡c cÃ¢u há»i phÃ¡p lÃ½ dá»±a trÃªn vÄƒn báº£n báº¡n cung cáº¥p."
)

# Táº£i cÃ¡c model cáº§n thiáº¿t
summarizer = load_summarizer_model()
embeddings = load_embedding_model()

# Táº¡o cÃ¡c tab cho tá»«ng chá»©c nÄƒng
tab1, tab2 = st.tabs(["ğŸ“ TÃ³m Táº¯t VÄƒn Báº£n", "ğŸ’¬ Há»i-ÄÃ¡p PhÃ¡p LÃ½ (RAG)"])

# --- Tab 1: TÃ³m táº¯t vÄƒn báº£n ---
with tab1:
    st.header("TÃ³m Táº¯t Báº£n Ãn Tá»± Äá»™ng")
    st.write("DÃ¡n toÃ n bá»™ ná»™i dung cá»§a má»™t báº£n Ã¡n vÃ o Ã´ dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ nháº­n báº£n tÃ³m táº¯t.")

    input_text = st.text_area(
        "Ná»™i dung báº£n Ã¡n:", height=300, placeholder="DÃ¡n ná»™i dung vÃ o Ä‘Ã¢y..."
    )

    if st.button("Táº¡o TÃ³m Táº¯t"):
        if summarizer:
            if input_text:
                summary_result = summarize_text(summarizer, input_text)
                st.subheader("Báº£n tÃ³m táº¯t:")
                st.write(summary_result)
            else:
                st.warning("Vui lÃ²ng nháº­p ná»™i dung báº£n Ã¡n.")
        else:
            st.error(
                "MÃ´ hÃ¬nh tÃ³m táº¯t chÆ°a Ä‘Æ°á»£c táº£i. Vui lÃ²ng kiá»ƒm tra láº¡i Ä‘Æ°á»ng dáº«n vÃ  file model."
            )

# --- Tab 2: Há»i-Ä‘Ã¡p ---
with tab2:
    st.header("Há»i-ÄÃ¡p Dá»±a TrÃªn TÃ i Liá»‡u")
    st.write(
        "Táº£i lÃªn má»™t hoáº·c nhiá»u file vÄƒn báº£n (.txt) Ä‘á»ƒ lÃ m cÆ¡ sá»Ÿ kiáº¿n thá»©c, sau Ä‘Ã³ Ä‘áº·t cÃ¢u há»i vá» ná»™i dung cá»§a chÃºng."
    )

    uploaded_files = st.file_uploader(
        "Táº£i lÃªn cÃ¡c file .txt cá»§a báº¡n", type="txt", accept_multiple_files=True
    )

    if uploaded_files:
        documents_content = []
        for file in uploaded_files:
            documents_content.append(file.read().decode("utf-8"))

        st.success(f"ÄÃ£ táº£i lÃªn vÃ  xá»­ lÃ½ {len(uploaded_files)} file.")

        # Chá»‰ xÃ¢y dá»±ng láº¡i há»‡ thá»‘ng QA náº¿u file Ä‘Æ°á»£c táº£i lÃªn thay Ä‘á»•i
        # Streamlit sáº½ tá»± Ä‘á»™ng cache káº¿t quáº£ cá»§a hÃ m nÃ y
        @st.cache_data
        def get_qa_chain(_docs_content):
            with st.spinner("Äang xÃ¢y dá»±ng cÆ¡ sá»Ÿ tri thá»©c tá»« tÃ i liá»‡u..."):
                return setup_qa_chain_from_text(_docs_content, embeddings)

        qa_chain = get_qa_chain(tuple(documents_content))  # DÃ¹ng tuple Ä‘á»ƒ cÃ³ thá»ƒ cache

        if qa_chain:
            st.info("Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng. HÃ£y Ä‘áº·t cÃ¢u há»i cá»§a báº¡n vÃ o Ã´ bÃªn dÆ°á»›i.")
            question = st.text_input("CÃ¢u há»i cá»§a báº¡n:")

            if question:
                with st.spinner("Äang tÃ¬m kiáº¿m cÃ¢u tráº£ lá»i..."):
                    start_time = time.time()
                    result = qa_chain.invoke({"query": question})
                    end_time = time.time()

                    st.subheader("âœ… CÃ¢u tráº£ lá»i:")
                    st.write(result["result"])
                    st.write(f"*(Thá»i gian xá»­ lÃ½: {end_time - start_time:.2f} giÃ¢y)*")

                    with st.expander("ğŸ” Xem cÃ¡c nguá»“n trÃ­ch dáº«n"):
                        for doc in result["source_documents"]:
                            st.write(
                                f"**TrÃ­ch tá»«:** {doc.page_content[:250].strip()}..."
                            )
